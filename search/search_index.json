{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to the Kubernetes Monitoring and Logging Bootcamp using Nutanix Karbon Kubernetes clusters. This bootcamp accompanies an instructor-led session that introduces Nutanix and Kubernetes technologies and many common management tasks.</p> <p>You will explore Prism Element and become familiar with its features and navigation. You will use Prism to perform basic cluster administration tasks, including storage and networking.</p> <p>At the end of the bootcamp, attendees should understand the Core concepts and technologies that make up the Nutanix Enterprise Cloud stack and Karbon offering and should be well prepared for a hosted or onsite proof-of-concept (POC) engagement.</p> <p>Note</p> <p>Estimated time to complete this lab is 90 minutes</p>"},{"location":"#whats-new","title":"What's New","text":"<ul> <li>Workshop updated for the following software versions:<ul> <li>AOS 5.20.3.x &amp; </li> <li>PC pc.2022.4.0.1</li> </ul> </li> </ul>"},{"location":"#lab-agenda","title":"Lab Agenda","text":"<p>Nutanix Karbon deployed kubernetes clusters come with Prometheus pre-installed and ready to use. However, Grafana is not available by default. We will go through the following steps to enable and visualize the status of the kubernetes cluster.</p> <ul> <li>Connect to Karbon deployed kubernetes cluster</li> <li>Explore the <code>ntnx-system</code> namespace for monitoring and logging resources</li> <li>Explore available Prometheus metrics</li> <li>Deploy Grafana in <code>ntnx-system</code> namespace</li> <li>Configure prometheus as a data source in Grafana</li> <li>Configure a custom dashboard</li> <li>Import a pre-configured dashboard</li> </ul>"},{"location":"#initial-setup","title":"Initial Setup","text":"<ul> <li>Take note of the Passwords being used.</li> <li>Log into your virtual desktops (connection info below)</li> </ul>"},{"location":"#whats-new_1","title":"What's New","text":"<ul> <li>Workshop uses for the following software versions:<ul> <li>AOS 5.20.2.1</li> <li>Prism Central pc.2021.9.0.2</li> <li>Calm 3.3.1</li> </ul> </li> </ul>"},{"location":"#initial-setup_1","title":"Initial Setup","text":"<ul> <li>Take note of the Passwords being used from you RX reservation     details</li> <li>Log into your virtual desktops (connection info below)</li> <li>Login to Global Protect VPN if you have access</li> </ul>"},{"location":"#cluster-assignment","title":"Cluster Assignment","text":"<p>The instructor will inform the attendees their assigned clusters.</p> <p>Note<p>If these are Single Node Clusters (SNCs) pay close attention on the networking part. The SNCs are setup and configured differently to the 3 or 4 node clusters</p> </p>"},{"location":"#environment-details","title":"Environment Details","text":"<p>Nutanix Workshops are intended to be run in the Nutanix Hosted POC environment. Your cluster will be provisioned with all necessary images, networks, and VMs required to complete the exercises.</p>"},{"location":"#networking","title":"Networking","text":"<p>As we are able to provide three/four node clusters and single node clusters in the HPOC environment, we need to describe each sort of cluster separately. The clusters are setup and configured differently.</p>"},{"location":"#threefour-node-hpoc-clusters","title":"Three/Four node HPOC clusters","text":"<p>Three or four node Hosted POC clusters follow a standard naming convention:</p> <ul> <li>Cluster Name - POCXYZ</li> <li>Subnet - 10.42.XYZ.0</li> <li>Cluster IP - 10.42.XYZ.37</li> </ul> <p>For example:</p> <ul> <li>Cluster Name - POC055</li> <li>Subnet - 10.42.55.0</li> <li>Cluster IP - 10.42.55.37 for the VIP of the Cluster</li> </ul> <p>Throughout the Workshop there are multiple instances where you will need to substitute XYZ with the correct octet for your subnet, for example:</p> IP Address Description 10.42.XYZ.37 Nutanix Cluster Virtual IP 10.42.XYZ.39 PC VM IP, Prism Central 10.42.XYZ.41 DC VM IP, NTNXLAB.local Domain Controller <p>Each cluster is configured with 2 VLANs which can be used for VMs:</p> Network Name Address VLAN DHCP Scope Primary 10.42.XYZ.1/25 0 10.42.XYZ.50-10.42.XYZ.124 Secondary 10.42.XYZ.129/25 XYZ1 10.42.XYZ.132-10.42.XYZ.253"},{"location":"#single-node-hpoc-clusters","title":"Single Node HPOC Clusters","text":"<p>For some workshops we are using Single Node Clusters (SNC). The reason for this is to allow more people to have a dedicated cluster but still have enough free clusters for the bigger workshops including those for customers.</p> <p>The network in the SNC config is using a /26 network. This splits the network address into four equal sizes that can be used for workshops. The below table describes the setup of the network in the four partitions. It provides essential information for the workshop with respect to the IP addresses and the services running at that IP address.</p> Partition 1 Partition 2 Partition 3 Partition 4 Service Comment 10.38.x.1 10.38.x.65 10.38.x.129 10.38.x.193 Gateway 10.38.x.5 10.38.x.69 10.38.x.133 10.38.x.197 AHV Host 10.38.x.6 10.38.x.70 10.38.x.134 10.38.x.198 CVM IP 10.38.x.7 10.38.x.71 10.38.x.135 10.38.x.199 Cluster IP 10.38.x.8 10.38.x.72 10.38.x.136 10.38.x.200 Data Services IP 10.38.x.9 10.38.x.73 10.38.x.137 10.38.x.201 Prism Central IP 10.38.x.11 10.38.x.75 10.38.x.139 10.38.x.203 AutoDC IP(DC) 10.38.x.32-37 10.38.x.96-101 10.38.x.160-165 10.38.x.224-229 Objects 1 10.38.x.38-58 10.38.x.102-122 10.38.x.166-186 10.38.x.230-250 Primary 6 Free IPs for static assignment"},{"location":"#credentials","title":"Credentials","text":"<p>Note<p>The Cluster Password is unique to each cluster and will be provided by the leader of the Workshop.</p> </p> Credential Username Password Prism Element admin Cluster Password Prism Central admin Cluster Password Controller VM nutanix Cluster Password Prism Central VM nutanix Cluster Password <p>Each cluster has a dedicated domain controller VM, DC, responsible for providing AD services for the NTNXLAB.local domain. The domain is populated with the following Users and Groups:</p> Group Username(s) Password Administrators Administrator nutanix/4u SSP Admins adminuser01-adminuser25 nutanix/4u SSP Developers devuser01-devuser25 nutanix/4u SSP Consumers consumer01-consumer25 nutanix/4u SSP Operators operator01-operator25 nutanix/4u SSP Custom custom01-custom25 nutanix/4u Bootcamp Users user01-user25 nutanix/4u"},{"location":"#access-instructions","title":"Access Instructions","text":"<p>The Nutanix Hosted POC environment can be accessed a number of different ways:</p>"},{"location":"#lab-access-user-credentials","title":"Lab Access User Credentials","text":"<p>PHX Based Clusters: </p> <ul> <li>Username: PHX-POCxxx-User01 (up to PHX-POCxxx-User20), </li> <li>Password: Provided by Instructor</li> </ul> <p>RTP Based Clusters: </p> <ul> <li>Username: RTP-POCxxx-User01 (up to RTP-POCxxx-User20), </li> <li>Password: Provided by Instructor</li> </ul>"},{"location":"#frame-vdi","title":"Frame VDI","text":"<p>Login to: https://console.nutanix.com/x/labs</p> <p>Nutanix Employees - Use your NUTANIXDC credentials Non-Employees - Use Lab Access User Credentials</p>"},{"location":"#parallels-vdi","title":"Parallels VDI","text":"<p>PHX Based Clusters Login to: https://xld-uswest1.nutanix.com</p> <p>RTP Based Clusters Login to: https://xld-useast1.nutanix.com</p> <p>Nutanix Employees - Use your NUTANIXDC credentials Non-Employees - Use Lab Access User Credentials</p>"},{"location":"#employee-pulse-secure-vpn","title":"Employee Pulse Secure VPN","text":"<p>Download the client:</p> <p>PHX Based Clusters Login to: https://xld-uswest1.nutanix.com</p> <p>RTP Based Clusters Login to: https://xld-useast1.nutanix.com</p> <p>Nutanix Employees - Use your NUTANIXDC credentials Non-Employees - Use Lab Access User Credentials</p> <p>Install the client.</p> <p>In Pulse Secure Client, Add a connection:</p> <p>For PHX:</p> <ul> <li>Type - Policy Secure (UAC) or Connection Server</li> <li>Name - X-Labs - PHX</li> <li>Server URL - xlv-uswest1.nutanix.com</li> </ul> <p>For RTP:</p> <ul> <li>Type - Policy Secure (UAC) or Connection Server</li> <li>Name - X-Labs - RTP</li> <li>Server URL - xlv-useast1.nutanix.com</li> </ul>"},{"location":"#nutanix-version-info","title":"Nutanix Version Info","text":"<ul> <li>AOS 5.20.3.x &amp; </li> <li>PC pc.2022.4.0.1</li> <li>Calm 3.3.1</li> </ul>"},{"location":"appendix/create_kube/","title":"Creating a Kubernetes Cluster","text":""},{"location":"appendix/create_kube/#overview","title":"Overview","text":"<p>Before we can deploy an application using kubernetes we need to create a Kubernetes cluster first. The cluster that we are ging to create consists out of the following VMs:</p> <ul> <li>1 Master node (VM)</li> <li>1 Worker node (VM)</li> <li>1 etcd nodes (VM)</li> </ul> <p>Info<p>For more information on the terms master, worker and etcd, please look at https://kubernetes.io/docs/concepts/.</p> </p>"},{"location":"appendix/create_kube/#creating-a-karbon-cluster","title":"Creating a Karbon Cluster","text":"<ol> <li> <p>In your Prism Central, select the three dash in the top left corner and select Services &gt; Karbon</p> <p></p> </li> <li> <p>A new browser window will open and accept the HTTPS error you will see.</p> <p></p> </li> <li> <p>In the shown browser screen click on the + Create Cluster button for starting to create the kubernetes cluster</p> <p></p> </li> <li> <p>Provide the required parameters that are asked for in the wizard. The following screenshots can be used as a guideline</p> <p>Tip</p> <p>You can also hover over the ? sign to get more information about each configuration option.</p> </li> <li> <p>Choose Development Cluster</p> <p></p> </li> <li> <p>Enter a name for your cluster initials-cluster and choose your HPOC as Nutanix Cluster</p> <p></p> </li> <li> <p>Provide the Worker, Master and etcd settings as default; click Next</p> <p></p> </li> <li> <p>Provide the Network Provider settings as default; click Next</p> <p>Info<p>We use flannel as the network provider. More information on Flannel can be found here</p> </p> <p></p> </li> <li> <p>Provide the Storage class settings. For the cluster settings use admin and the cluster password that you used to login to the cluster.</p> <p></p> </li> <li> <p>Click on the Create button to have the cluster created by the system. Follow the process in the Karbon UI.</p> <p>Note<p>Based on the resources available on your cluster, it will take time. Wait until the cluster has been created before proceeding to the next part of the module!!</p> </p> <p></p> </li> </ol> <p>During the creation of the Kubernetes cluster there will have been created:</p> <ul> <li> <p>VMs</p> <p></p> </li> <li> <p>Persistent Storage as VolumeGroup</p> <p></p> <p></p> </li> </ul>"},{"location":"appendix/create_kube/#cluster-properties","title":"Cluster properties","text":"<p>In the Karbon UI, hover over the just created cluster (initials-cluster in our example) and click on it.</p> <p></p> <p>This will open another screen which shows the parts out of which the cluster is created according to the provided parameters that have been provided during the creation phase.</p> <p></p> <p>The below screenshots provide an example for the three parts, Storage Class, Volume and Add-on.</p> <p></p> <p></p> <p></p> <p>This concludes the end of this part of the module. You now have a running Kubernetes Cluster.</p>"},{"location":"appendix/create_kube/#takeaways","title":"Takeaways","text":"<ul> <li>It is quite easy to create a kubernetes cluster using Nutanix Karbon</li> <li>Both Development and Production cluster with different node count     options are available</li> <li>Flannel is the only CNI option available in the Karbon GUI. However,     Calico is avaialble to configure through REST API calls</li> </ul>"},{"location":"appendix/glossary/","title":"Glossary","text":"<p>.. _glossary:</p> <p>Glossary</p> <p>Nutanix Core ++++++++++++</p> <p>AOS ...</p> <p>AOS stands for Acropolis Operating System, and it is the OS running on the Controller VMs (CVMs).</p> <p>Pulse .....</p> <p>Pulse provides diagnostic system data to Nutanix customer support teams so that they can deliver proactive, context-aware support for Nutanix solutions.</p> <p>Prism Element .............</p> <p>Prism Element is the native management plane for Nutanix. Because its design is based on consumer product interfaces, it is more intuitive and easier to use than many enterprise application interfaces.</p> <p>Prism Central .............</p> <p>Prism Central is the multicloud control and management interface for Nutanix. Prism Central can manage multiple Nutanix clusters and serves as an aggregation point for monitoring and analytics.</p> <p>Node ....</p> <p>Industry standard x86 server with server-attached SSD and optional HDD (All Flash &amp; Hybrid Options).</p> <p>Block .....</p> <p>2U rack mount chassis that contains 1, 2 or 4 nodes with shared power and fans, and no shared no backplane.</p> <p>Storage Pool ............</p> <p>A storage pool is a group of physical storage devices including PCIe SSD, SSD, and HDD devices for the cluster.</p> <p>Storage Container .................</p> <p>A container is a subset of available storage used to implement storage policies.</p> <p>Anatomy of a Read I/O .....................</p> <p>Performance and Availability</p> <ul> <li>Data is read locally</li> <li>Remote access only if data is not locally present</li> </ul> <p>Anatomy of a Write I/O ......................</p> <p>Performance and Availability</p> <ul> <li>Data is written locally</li> <li>Replicated on other nodes for high availability</li> <li>Replicas are spread across cluster for high performance</li> </ul> <p>Nutanix Karbon +++++++++++++++</p> <p>Nutanix Karbon is an enterprise-grade Kubernetes Certified distribution that simplifies the provisioning, operations and lifecycle management of Kubernetes clusters with a native Kubernetes experience. Karbon makes it simple to deploy and maintain highly available Kubernetes cluster and operate web-scale workloads.</p>"},{"location":"appendix/helm/","title":"Helm","text":"<p>Helm is a package manager for Kubernetes based applications and resources deployment.</p> <p>Helm also provides an easy way of accomplishing the following:</p> <ul> <li>Install and uninstall kubernetes applications</li> <li>Versioning complex kubernetes applications</li> <li>Upgrade and rollback kubernetes applications</li> </ul> <p>You can think of Helm as <code>yum</code> or <code>apt-get</code> package managers in Linux.</p> <p>Helm has the following main components (not limited to):</p> <ul> <li>Helm command line tool - provides interface to all Helm functionality</li> <li>Charts - charts are manifests (yaml) files for desired kubernetes resources</li> <li>Value file(s) - all customisable values for the application deployment can be configured here</li> </ul> <p>Note<p>Helm used to be a server-client application. Helm being the client and Tiller server component being installed in a kubernetes cluster. With Helm 3.x onwards, Tiller is removed and only Helm is used for ease.</p> </p>"},{"location":"appendix/helm/#installing-helm-3x","title":"Installing Helm 3.x","text":"<p>Use the following commands to install helm in your LinuxTools VM.</p> <p>Install latest Helm through a script<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> Verify Helm version<pre><code>helm version\n</code></pre></p>"},{"location":"appendix/linux_tools_vm/","title":"Linux Tools VM","text":""},{"location":"appendix/linux_tools_vm/#overview","title":"Overview","text":"<p>Deploy this VM on your assigned cluster (if not already deployed).</p> <p>Caution<p>Only deploy the VM once with your Initials in the VM name, it does not need to be cleaned up as part of any lab completion.</p> </p>"},{"location":"appendix/linux_tools_vm/#deploying-linux-tools-vm","title":"Deploying Linux Tools VM","text":"<ol> <li> <p>In Prism Central &gt; select Menu &gt; Compute and Storage &gt; VMs, and click Create VM</p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>Name - Initials-Linux-ToolsVM</li> <li>Description - (Optional) Description for your VM.</li> <li>Number of VMs - 1</li> <li>CPU(s) - 4</li> <li>Number of Cores per CPU - 1</li> <li>Memory - 4 GiB</li> </ul> </li> <li>Click Next</li> <li>Under Disks select Attach Disk<ul> <li>Type - DISK</li> <li>Operation - Clone from Image</li> <li>Image - Linux_ToolsVM.qcow2</li> <li>Capacity - leave at default size</li> <li>Bus Type - leave at default SCSI Setting</li> </ul> </li> <li>Click Save</li> <li>Under Networks select Attach to Subnet<ul> <li>VLAN Name - Primary</li> <li>Network Connection State - Connected</li> <li>Assignment Type - Assign with DHCP</li> </ul> </li> <li>Click Save</li> <li>Click Next at the bottom</li> <li> <p>In Management section</p> <ul> <li>Categories - leave blank</li> <li>Timezone - leave at default UTC</li> <li> <p>Guest Customization - </p> <ul> <li>Script Type - Cloud-init (Linux)</li> <li>Configuration Method - Custom Script </li> </ul> <p>Do you need to create a SSH key pair?</p> <p>You can use any online ssh key generator if you are using Windows. Execute the following commands in you are in a Linux / Mac environment to generate a private key.</p> <p><pre><code>ssh-keygen -t rsa -b 2048 -C \"Created for Linux Tools VM\"\n# follow prompts \n# do not specify passphrase\n# once completed run the following command\ncat id_rsa.pub\n\n# copy the contents of the id_rsa.pub file to your cloudinit yaml file\n</code></pre> </p> <ul> <li>Paste the following script in the script window </li> </ul> <pre><code>#cloud-config\n# Set the hostname\nhostname: myhost\n# Create a new user\nusers:\n- default\n- name: nutanix\ngroups: users\nssh_authorized_keys:\n# Paste the generated public key here\n- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD...... # You can also use the 1N or 6N format (openssl passwd -1 \"yourplaintextpassword\")\npasswd: $1$CW55TPkL$gzOS35bI6fdxtwntlE3CN/                             # Enable password authentication for root\nssh_pwauth: True\n# Run additional commands\nruncmd:\n- 'sleep 10' # sleeping for the network to be UP\n- 'echo \"nutanix ALL=(ALL) NOPASSWD: ALL\" &gt;&gt; /etc/sudoers'\n# Run package upgrade\npackage_upgrade: true\n# Install the following packages - add extra that you would need\npackages:\n- git\n- bind-utils\n- nmap\n- curl\n- wget - vim\n</code></pre> <li> <p>Click on Next</p> </li> <li>Click Create VM at the bottom</li> <li>Go back to Prism Central &gt; Menu &gt; Compute and Storage &gt; VMs</li> <li>Select your Initials-Linux-ToolsVM</li> <li> <p>Under Actions drop-down menu, choose Power On</p> <p>Note<p>It may take up to 10 minutes for the VM to be ready.</p> <p>You can watch the console of the VM from Prism Central to make sure all the clouinit script has finished running.</p> </p> </li> <li> <p>Login to the VM via SSH or Console session, using the following command:</p> <p><pre><code>ssh -i &lt;your_private_key&gt; -l nutanix &lt;IP of LinuxToolsVM&gt;\n</code></pre> Example command<pre><code>ssh -i id_rsa -l nutanix 10.54.63.95\n</code></pre></p> </li>"},{"location":"appendix/privatereg/","title":"Using Public Container Registry","text":"<p>In this section we will go through a few steps to use your own registry and associated credentials.</p> <p>Since most public container registries shape/rate-limit download, it may be essential to you use a private registry or use your login credentials to download container images from a public registry (Docker, etc).</p> <p>In kubernets, we can configure the service accounts to use your public registry credentials to download container images. The steps include the following:</p> <ol> <li>Create a generic kubernetes secret in your namespace with public registry credentials - this will become your pull secret</li> <li>Associate a kubernetes service account to use your pull secret</li> </ol>"},{"location":"appendix/privatereg/#create-a-container-pull-secret","title":"Create a Container Pull Secret","text":"<ol> <li>Login to your Linux Tools VM</li> <li>Authenticate to your kubernetes cluster using the kubeconfig file your downloaded from Karbon</li> <li>Change to the namespace you would like to use    <pre><code>kubectl config set-context --current --namespace=&lt;your-namespace&gt;\n</code></pre> Example command<pre><code>kubectl config set-context --current --namespace=default\n</code></pre></li> <li>Create a pull secret using your Docker account    <pre><code>kubectl create secret docker-registry regcred \\\n--docker-username=DUMMY_USERNAME --docker-password=DUMMY_DOCKER_PASSWORD \\\n--docker-email=DUMMY_DOCKER_EMAIL\n</code></pre></li> </ol>"},{"location":"appendix/privatereg/#associate-pull-secret-with-service-account","title":"Associate Pull Secret with Service Account","text":"<ol> <li>Associate pull secret with the service account your will be using    <pre><code>kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"regcred\"}]}'\n</code></pre></li> <li> <p>Verify if the serviceaccount is using your <code>regcred</code> docker registy secret</p> <pre><code>kubectl get sa default -o yaml\n</code></pre> <p>Command output<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: default\nnamespace: default\nuid: 052fb0f4-3d50-11e5-b066-42010af0d7b6\nimagePullSecrets:\n- name: regcred\n</code></pre> 6. Now your yaml manifest can use this secret to download container images</p> <p>Create a pod<pre><code>k run nginx --image=nginx --restart=Never\n</code></pre> <pre><code>k get po nginx -oyaml\n</code></pre> Command output if the pod was created by default service account<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nlabels:\nrun: nginx\nname: nginx\nspec:\ncontainers:\n- image: nginx\nname: nginx\ndnsPolicy: ClusterFirst\nrestartPolicy: Never\nimagePullSecrets:\n- name: regcred\n</code></pre> Delete the pod<pre><code>k delete po nginx\n</code></pre></p> </li> </ol> <p>You can now use your docker credentials to download container images from Docker public registry.</p>"},{"location":"cleanup/cleanup/","title":"Cleanup and Takeaways","text":""},{"location":"cleanup/cleanup/#cleanup","title":"Cleanup","text":"<p>Cleanup port-forwarding for Prometheus and Grafana. Just ctrl+c the commands.</p> <p>If you have sent these commands to background, you can bring them to foreground by using</p> <pre><code>fg\n</code></pre> <p>Cleanup your namespace. This will also delete all the resources inside the namespace.</p> <p>Change to ntnx-system to be able to list deployed helm charts in that namespace<pre><code>k config set-context $(k config current-context) --namespace=ntnx-system\n</code></pre> Copy your grafana chart name to use in the next uninstall command<pre><code>helm list  NAME NAMESPACE REVISION APP VERSION grafana-1597884244  ntnx-system 1 7.1.1\n</code></pre> This chart name will vary in your implementation<pre><code>helm uninstall grafana-1597884244 </code></pre></p> <p>Note</p> <p>Do not delete Prometheus implementation in ntnx-system namespace. Leave it as is.</p>"},{"location":"cleanup/cleanup/#takeaways","title":"Takeaways","text":"<p>We have been through implementation and use of kubernetes monitoring and logging in this bootcamp and we can't help but notice that this process is quite simple.</p> <ul> <li>Prometheus is open-source metrics collector</li> <li>Nutanix Karbon deployed kubernetes has a default implementation of Prometheus to monitor the health of kubernetes nodes and appplications</li> <li>It is a good practice to deploy a separate instance of Prometheus to monitor user applications</li> <li>Grafana is a visualisation tool which we could deploy to visualise collected metrics</li> <li>Elastic Stack is open-source logging collector, analyser and visualisation framework</li> <li>Nutanix Karbon deployed kubernetes has a default implementation of Elastic Stack (Elastisearch and Kibana), and Fluent Bit for logs processing</li> <li>Kibana is the log visualisation tool of choice in the Elastic Stack</li> <li>Customers are able to deploy their own instance of Elastic Stack for their applications</li> <li>Kubernetes operators are a easy way of installing, upgrading, life-cycle managing a complex stateful application</li> </ul>"},{"location":"connect/connect/","title":"Installing Grafana in Karbon","text":"<p>In this exercise we will install Grafana into the same <code>ntnx-system</code> namespace using Helm. </p> <p>If you haven't got Helm deployed use these instructions to deploy it in your Linux Tools VM.</p>"},{"location":"connect/connect/#overview","title":"Overview","text":"<ol> <li>Connect to Linux Tools VM (if not already connected)</li> <li>Install Grafana into <code>ntnx-system</code> namespace</li> </ol>"},{"location":"connect/connect/#connect-to-your-linux-tools-vm","title":"Connect to your Linux Tools VM","text":"<ol> <li> <p>Logon to your Linux Tools VM console as <code>root</code> user (default password) and open terminal.</p> <p>Info</p> <p>If you are using your PC/Mac you can also ssh/putty to your Linux Tools VM</p> <pre><code>ssh -l root &lt;Linux Tools VM IP address&gt;\n</code></pre> </li> </ol>"},{"location":"connect/connect/#install-grafana","title":"Install Grafana","text":"<ol> <li> <p>Add the helm repos of Grafana</p> <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre> </li> <li> <p>We will chech which tool we should add for Grafana from the repo</p> <p><pre><code>helm search repo grafana\n</code></pre> Output - we can see that we need to use the first tool<pre><code>NAME                                    CHART VERSION   APP VERSION         DESCRIPTION                                       \ngrafana/grafana                         6.44.11         9.3.0               The leading tool for querying and visualizing t...\ngrafana/grafana-agent-operator          0.2.8           0.28.0              A Helm chart for Grafana Agent Operator           \ngrafana/enterprise-logs                 2.4.2           v1.5.2              Grafana Enterprise Logs                           \ngrafana/enterprise-logs-simple          1.2.1           v1.4.0              DEPRECATED Grafana Enterprise Logs (Simple Scal...\ngrafana/enterprise-metrics              1.9.0           v1.7.0              DEPRECATED Grafana Enterprise Metrics             \ngrafana/fluent-bit                      2.3.2           v2.1.0              Uses fluent-bit Loki go plugin for gathering lo...\ngrafana/loki                            3.5.0           2.6.1               Helm chart for Grafana Loki in simple, scalable...\n</code></pre></p> </li> <li> <p>Create a values files to instruct Helm chart to use <code>default</code> service account and specify other parameters of install</p> <pre><code>cat &lt;&lt; EOF &gt; values.yaml\nserviceAccount:\n  create: false\n  name: default # &gt;&gt; The namespace's default sa will be used to install Grafana\npersistence:\n  type: pvc\n  enabled: true\n  size: 10Gi  # &gt;&gt; Grafana will use this 10 Gi storage  \nservice:\n  enabled: true\n  type: NodePort # &gt;&gt; Grafana will be available on a NodePort\nEOF\n</code></pre> </li> <li> <p>Install Grafana using Helm and using <code>values.yaml</code> file</p> <p><pre><code>helm install grafana/grafana --generate-name --namespace ntnx-system \\\n-f values.yaml\n</code></pre> You will see command output as follows:</p> <p></p> <p>Tip<p>In case there are issues with downloading the pod from Docker hub, follow the instructions here to set your service account of choice to use a Docker registry secret containing your Docker public hub credentials.</p> </p> </li> <li> <p>Now let's get the password for Grafana implementation using which we can logon to Grafana console</p> <p><pre><code>kubectl get secret --namespace grafana grafana-1669965734 \\\n-o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n</code></pre> Output for password - this will be different for you<pre><code>waeIaO6AuKWW2x7aqnOyzCRnU6GIVQRvDLltm2Qr\n</code></pre></p> </li> <li> <p>Get the Grafana URL to visit by running these commands in the same shell</p> <p>Get the Grafana URL<pre><code>export NODE_PORT=$(kubectl get --namespace grafana -o jsonpath=\"{.spec.ports[0].nodePort}\" services grafana-1669965734)\nexport NODE_IP=$(kubectl get nodes --namespace grafana -o jsonpath=\"{.items[0].status.addresses[0].address}\")\necho http://$NODE_IP:$NODE_PORT\n</code></pre> Output - be sure to use your IP addresses<pre><code>http://10.38.9.180:31800\n</code></pre></p> </li> <li> <p>Login to Grafana in a browser using the access URL and password from     the previous steps</p> <p></p> <p></p> </li> </ol> <p>This completes your Grafana installation.</p>"},{"location":"connect/connect/#configure-grafana","title":"Configure Grafana","text":"<p>We will now configure the following in Grafana to visualise the health and status of Karbon kubernetes nodes, resources and some applications.</p> <p>To be able to set up views, we need to do the following:</p> <ul> <li>Setting up a data source for Grafana - Prometheus in our case</li> <li>Test data source for Grafana</li> </ul> <p>Once the data source is configured we will do the following:</p> <ul> <li>Configure a custom dashboard</li> <li>Import a Grafana community configured dashboard</li> </ul>"},{"location":"connect/connect/#setting-up-a-data-source","title":"Setting up a Data Source","text":"<ol> <li> <p>In Grafana UI, click on Data Sources (Add your first data source)</p> <p></p> </li> <li> <p>Select Prometheus</p> <p></p> </li> <li> <p>Enter the following in the datasource URL in the URL-field and click     on Save and Test</p> <pre><code>http://prometheus-k8s.ntnx-system.svc.cluster.local:9090\n</code></pre> <p></p> <p>Note</p> <p>For the datasource URL above; the following are the parts of URL prometheus-k8s     - name of your Prometheus service ntnx-system - your namespace svc - your Prometheus service cluster.local - generic DNS name for your kubernetes cluster 9090 - prometheus ClusterIP port</p> <p>This is the DNS reference of the Prometheus service within your namespace. All services in the namespace are able to resolve by doing a DNS lookup with kube-dns DNS server.</p> </li> <li> <p>You should see a Data source is working message to confirm. If     this is not working check for typos in the datasource URL.</p> <p></p> </li> </ol> <p>We have now configured a datasource for Grafan. Let's move on to configuring dashboards and visualising metrics in the next section.</p>"},{"location":"explore/explore/","title":"Exploring Monitoring Resources","text":"<p>In this exercise we will explore Prometheus into the same <code>ntnx-system</code> namespace using Helm. </p> <p>If you haven't got Helm deployed use these instructions to deploy it in your Linux Tools VM.</p>"},{"location":"explore/explore/#overview","title":"Overview","text":"<ol> <li>Create a Linux Tools VM (If one is not deployed already please use the instructions here to deploy one)</li> <li>Connect to Linux Tools VM and install kubectl tool</li> <li>Access you Karbon page and download KUBECONFIG file to Linux Tools VM</li> <li>Explore Grafana into <code>ntnx-system</code> namespace</li> </ol>"},{"location":"explore/explore/#connect-to-your-linux-tools-vm","title":"Connect to your Linux Tools VM","text":"<ol> <li> <p>Logon to your Linux Tools VM console as <code>root</code> user (default password) and open terminal.</p> <p>Info</p> <p>If you are using your PC/Mac you can also ssh/putty to your Linux Tools VM</p> <pre><code>ssh -i &lt;your_private_key&gt; -l nutanix &lt;IP of LinuxToolsVM&gt;\n</code></pre> </li> <li> <p>Paste the command in clipboard to the shell in your Linux Tools VM to install kubectl</p> <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre> </li> <li> <p>Verify your kubectl installation</p> <pre><code>alias 'k=kubectl' k version --client\n</code></pre> </li> </ol>"},{"location":"explore/explore/#access-your-kubernetes-cluster","title":"Access your Kubernetes Cluster","text":"<ol> <li> <p>Logon to your Prism Central <code>https://PC-VM-IP:9440</code></p> <p>Caution<p>If you haven't got a Karbon deployed kubernetes cluster in your HPOC, refer here to create it before proceeding with this section of the lab.</p> </p> </li> <li> <p>Go to Menu &gt; Services &gt; Karbon</p> <p></p> </li> <li> <p>Select your karbon cluster</p> </li> <li> <p>Click on Actions &gt; Download Kubeconfig</p> <p></p> </li> <li> <p>Click on Copy the command to clipboard</p> </li> <li> <p>Paste the contents in your Linux Tools VM shell</p> </li> <li> <p>Run the following command to verify your connectivity and display     the nodes in the cluster</p> <pre><code>k get nodes -o wide\n</code></pre> <p></p> </li> <li> <p>You can list the namespaces, storage claims, physical volumes and     physical volume claims using the following commands</p> <pre><code>k get ns k get sc,pv,pvc k get po -n ntnx-system\n</code></pre> <p></p> <p>Note<p>Nutanix Karbon has automatically provisioned these kubernetes resources so it is ready to use. You have the option to provision additional storage claims, physical volumes, etc by using the Karbon console or using kubectl with YAML files</p> </p> </li> </ol> <p>You can also notice that Prometheus pods are running in the <code>ntnx-system</code>. We will make use of this Prometheus implementation as a data source for Grafana.</p> <p>Now that you have an understanding of available kubernetes cluster resources, go ahead and install Grafana.</p>"},{"location":"explore/explore/#exploring-prometheus-in-karbon","title":"Exploring Prometheus in Karbon","text":"<p>In this section we will explore the <code>ntnx-system</code> namespace in a given Nutanix Karbon deployed kubernetes cluster and its resources in terms of Prometheus monitoring.</p> <p>If you have not got the Karbon K8S cluster deployed already, please use the deployment instructions here to deploy it before proceeding to the next section.</p>"},{"location":"explore/explore/#node-exporter","title":"Node Exporter","text":"<p>Node exporter is used to export metrics from each of the Kubernetes nodes. So a Kubernetes daemonset is used to accomplish this task.</p> <p>Daemonset makes sure there is an instance of node-exporter pod in each node of the kubernetes cluster as shown below.</p> <p></p> <p>In case the node-exporter pod fails, the kubernetes control loop will replace it with a new node-exporter pod at all times. So we are sure that metrics are being collected and uploaded to the Prometheus metrics store.</p>"},{"location":"explore/explore/#exploring-prometheus","title":"Exploring Prometheus","text":"<p>We will now check a few metrics that are collected and exposed by Promethues implementation that comes with Karbon deployed kubernetes clusters</p> <p>Let's access the Prometheus UI to check for available metrics.</p> <ol> <li> <p>Find the Prometheus Service's port number using the following     command.</p> <pre><code>k get svc -n ntnx-system\n</code></pre> <p></p> </li> <li> <p>Now let's forward Prometheus service's <code>9090</code> port to your Linux     Mint VM to be able to access Prometheus UI.</p> <pre><code>k port-forward svc/prometheus-k8s 9090:9090 &amp;\n</code></pre> </li> <li> <p>Now you should be able to access Prometheus UI in your Linux Mint VM     using the following URL</p> <pre><code>http://localhost:9090\n</code></pre> <p></p> </li> <li> <p>Explore available metrics by selecting insert metric at cursor     drop-down list. Here you will see a list of available metrics.</p> </li> <li> <p>Here we will choose go_memstats_alloc_bytes metrics and check if     I get any data. Note that the query text-box is now populated with     the query go_memstats_alloc_bytes</p> </li> <li> <p>Click on Execute. You will see the results of memory used by     each kubernetes resource (pod) as show in the following figure.</p> <p></p> </li> <li> <p>Notice that there is also a Graph option in Prometheus GUI for     the selected metrics. Click on Graph to visualise the chosen     metrics.</p> <p></p> </li> <li> <p>You can also type in a keyword in the query window and suggestions     for available metrics would show. You can see that typing the     keyword <code>node</code> has brought up all available node based metrics.</p> <p></p> </li> </ol> <p>In day to day operations, most of the metrics made available in the default implementation of Prometheus in Nutanix Karbon deployed kubernetes clusters. Highlight this fact to a customer during a design workshop.</p>"},{"location":"explore/explore/#prometheus-operator","title":"Prometheus Operator","text":"<p>As we already know Kubernetes has resources such as deployments and daemonsets (not limited to) for managing stateless applications, Operators are kubernetes extensions that will allow for stateful applications to be managed. As stateful applications need more domain specific knowledge of scaling, upgrading and reconfiguring, operators provide a way of doing this.</p> <p>Operators can be developed individually or obtained from a community maintaining these.</p> <p>For example: Prometheus as a stateful application has an operator and popular databases like MS SQL do too.</p> <p>Nutanix Karbon deployed clusters come with a Prometheus operator and it runs as a replica set maintaining at least one operator pod being alive at any given time. We can use this prometheus operator to deploy a separate instance of Prometheus to collect and maintain metrics from other applications in our Karbon deployed Kubernetes cluster.</p> <p>A good design would advocate the following for Prometheus implementation: You may come across this design question while working with a customer on a Karbon (Microservices) opportunity. Please take a moment to go through this.</p> <ol> <li>Use the Nutanix deployed prometheus instance to monitor nodes and     system namespaces</li> <li>Deploy separate instances of Prometheus collectors for user     applications and user namespaces using the Nutanix deployed     prometheus operator</li> </ol> <p>A view of this is illustrated in the following diagram:</p> <p></p> <p>Image Source: Prometheus Instances for Kubernetes and Applications</p> <p>Nutanix's very own DevOps Architect Christophe Jauffret has written a great article here to deploy a separate instance of Prometheus to monitor user applications over here. You could recommend this method to a prospective customer.</p> <p>Note<p>The steps in the Christophe's article above will work in your lab environment. This might take up to 45 minutes. Please only do this if you have extra time in this session.</p> </p>"},{"location":"logging/logging/","title":"Elastic Logging Kibana","text":"<p>In this lab, we will run through setting up logging with ELK Stack in your Karbon deployed kubernetes cluster using Helm. If you haven't setup Helm, use these Helm instructions to deploy it in your Linux Tools VM.</p> <p>This setup will collect logs from all applications deployed in Karbon kubernetes cluster.</p> <p>Info<p>All logs for kubernetes nodes (Master, ETCD and Workers) are collected by a separate instance of Elastisearch in the <code>ntnx-system</code> namespace. This is deployed by default in all Karbon kubernetes clusters.</p> </p> <p>The high level steps included in this lab are:</p> <ul> <li>Elasticsearch installation -     Elasticsearch is a distributed, open source search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured.</li> <li>Filebeat installation     is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to Elasticsearch or Logstash for indexing.</li> <li>Kibana installation - Kibana is an open source frontend application that sits on top of the Elastic Stack, providing search and data visualization capabilities for data indexed in Elasticsearch.</li> </ul> <p>In a production implementation make sure sufficient thought has been put in for design components of Elasticsearch in terms of:</p> <ul> <li>Log retention which directly affects storage requirements <ul> <li>Physical Volumes (PV) - provisioned by Nutanix Volumes</li> <li>In this implementation we will use a 30 GB PV (this is also customizable)</li> </ul> </li> <li>Log rotation</li> <li>Namespace requirements for ELK - it is best to have a separate namespace for logging implementation and define resource boundaries</li> </ul>"},{"location":"logging/logging/#connect-to-your-linux-tools-vm","title":"Connect to your Linux Tools VM","text":"<ol> <li> <p>Logon to your Linux Tools VM console as <code>root</code> user (default password) and open terminal.</p> <p>Info</p> <p>If you are using your PC/Mac you can also putty/ssh to your Linux Tools VM</p> <pre><code>ssh -l root &lt;Linux Tools VM IP address&gt;\n</code></pre> </li> <li> <p>Logon to your Prism Central <code>https://&lt;PC VM IP&gt;:9440</code></p> <p>Note<p>If you haven't got a Karbon deployed kubernetes cluster in your HPOC, refer here to create a kubernetes cluster in Nutanix.</p> </p> </li> <li> <p>Go to Menu &gt; Services &gt; Karbon</p> <p></p> </li> <li> <p>Select your Karbon cluster</p> </li> <li> <p>Click on Actions &gt; Download Kubeconfig</p> <p></p> </li> <li> <p>Click on Copy the command to clipboard</p> </li> <li> <p>Paste the contents in your Linux Tools VM shell</p> </li> <li> <p>Run the following command to verify your connectivity and display     the nodes in the cluster</p> <pre><code>alias 'k=kubectl'\nk get nodes -o wide\n</code></pre> <p></p> </li> <li> <p>You can list the namespaces, storage claims, physical volumes and     physical volume claims using the following commands</p> <p><pre><code>k get ns k get sc,pv,pvc k get po -n ntnx-system\n</code></pre> </p> <p>Info<p>Nutanix Karbon has automatically provisioned these kubernetes resources so it is ready to use. You have the option to provision additional storage claims, physical volumes, etc by using the Karbon console or using kubectl with YAML files</p> </p> </li> </ol>"},{"location":"logging/logging/#access-your-karbon-kubernetes-cluster","title":"Access your Karbon Kubernetes Cluster","text":""},{"location":"logging/logging/#install-elk-stack","title":"Install ELK Stack","text":"<p>We will install the following to get a working implementation of ELK Stack.</p>"},{"location":"logging/logging/#install-elasticsearch","title":"Install Elasticsearch","text":"<ol> <li> <p>Create a new namespace for your ELK stack</p> <pre><code>alias 'k=kubectl' k create ns elk #change default namespace to ELK k\nk config set-context --current --namespace=elk\n</code></pre> </li> <li> <p>If you would like to customise the size of PV and container     resources, configure a HELM values file</p> </li> <li> <p>Create a file using the content above and call it     <code>elastic_values.yaml</code></p> <pre><code>cat &lt;&lt;EOF &gt; elastic_values.yaml\n---\n# Elasticsearch roles that will be applied to this nodeGroup\n# These will be set as environment variables. E.g. node.master=true\nroles:\n  - master\n  - data\n  - ingest\nreplicas: 1\nminimumMasterNodes: 1\nclusterHealthCheckParams: 'wait_for_status=yellow&amp;timeout=1s'\n# Set to use default service account\nrbac:\n  create: false\n  serviceAccountName: \"default\"\n  automountToken: true\n# Shrink default JVM heap.\nesJavaOpts: \"-Xmx128m -Xms128m\"\n# Allocate smaller chunks of memory per pod.\nresources:\n    requests:\n        cpu: \"100m\"\n        memory: \"512M\"\n    limits:\n        cpu: \"1000m\"\n        memory: \"512M\"\n# Request smaller persistent volumes.\nvolumeClaimTemplate:\n    accessModes: [ \"ReadWriteOnce\" ]\n    resources:\n        requests:\n            storage: 30Gi\nEOF\n</code></pre> </li> <li> <p>Run the following command to install elasticsearch</p> <p><pre><code>helm install elasticsearch elastic/elasticsearch -f elastic_values.yaml\n</code></pre> You will see output as follows:<pre><code>helm install elasticsearch elastic/elasticsearch -f elastic_values.yaml\nNOTES:\n1. Watch all cluster members come up.\n$ kubectl get pods --namespace=elk -l app=elasticsearch-master -w\n2. Retrieve elastic user's password.\n$ kubectl get secrets --namespace=elk elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 -d\n3. Test cluster health using Helm test.\n$ helm --namespace=elk test elasticsearch\n</code></pre> Check if pods are coming up<pre><code>helm install elasticsearch elastic/elasticsearch -f elastic_values.yaml\n</code></pre> Retrieve elastic user's password<pre><code>k get secrets --namespace=elk elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 -d\n</code></pre> Test cluster health using Helm test<pre><code>helm --namespace=elk test elasticsearch\n</code></pre></p> <p>Tip<p>In case there are issues with downloading the pod from Docker hub, follow the instructions here to set your service account of choice to use a Docker registry secret containing your Docker public hub credentials.</p> </p> </li> <li> <p>Wait for the command to execute and check logs to make sure all your elasticsearch resources are running</p> <p>Check events to make sure there are no errors<pre><code>k get events\n</code></pre> Check all pods and other services are running<pre><code>k get all\n</code></pre> Output<pre><code># NAME                             READY   STATUS    RESTARTS   AGE\n# elasticsearch-master-0           1/1     Running   0          155m\n# elasticsearch-master-1           1/1     Running   0          155m\n# elasticsearch-master-2           1/1     Running   0          155m\n# NAME                                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\n# service/elasticsearch-master            ClusterIP   172.19.171.221   &lt;none&gt;        9200/TCP,9300/TCP   156m\n# service/elasticsearch-master-headless   ClusterIP   None             &lt;none&gt;        9200/TCP,9300/TCP   156m\n#\n# NAME                                    READY   AGE\n# statefulset.apps/elasticsearch-master   3/3     156m\n</code></pre></p> </li> <li> <p>Check the Physical Volumes to get an understanding of what is provisioned to to support Elasticsearch and its storage requirements - here it is 30 GB in capacity. This can be modified in the HELM values file.</p> <p>Check PVC resources<pre><code>k get pvc\n</code></pre> There will be three to support the three volumes - one for each pod and PV<pre><code># NAME                                          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           AGE\n# elasticsearch-master-elasticsearch-master-0   Bound    pvc-c8aad9f5-f24c-4e2e-917e-55107e072114   30Gi       RWO            default-storageclass   162m\n# elasticsearch-master-elasticsearch-master-1   Bound    pvc-141cc537-250d-472e-b686-c7dfafabf29a   30Gi       RWO            default-storageclass   162m\n# elasticsearch-master-elasticsearch-master-2   Bound    pvc-04302b11-a6e0-459c-8b74-0978f392df07   30Gi       RWO            default-storageclass   162m\n</code></pre> Check all the events to make sure there are no errors<pre><code>k get events\n</code></pre></p> </li> <li> <p>We have now installed Elasticsearch</p> </li> </ol>"},{"location":"logging/logging/#install-filebeat","title":"Install Filebeat","text":"<ol> <li> <p>Configure a values file using the following commands: this is     required to satisfy Karbon kubernetes cluster and volume mount     requirements</p> <pre><code>cat &lt;&lt;EOF &gt; filebeat_values.yaml\n---\nextraVolumeMounts:\n- name: varnutanix\n  mountPath: /var/nutanix\n  readOnly: true\nextraVolumes:\n- name: varnutanix\n  hostPath:\n    path: /var/nutanix\nEOF\n</code></pre> </li> <li> <p>Run the following command to install filebeat</p> <p><pre><code>helm install filebeat elastic/filebeat -f filebeat_values.yaml\n</code></pre> Output<pre><code>NAME: filebeat\nLAST DEPLOYED: Fri Dec  2 22:28:14 2022\nNAMESPACE: elk\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\n1. Watch all containers come up.\n$ kubectl get pods --namespace=elk -l app=filebeat-filebeat -w\n</code></pre> <pre><code>k get pods --namespace=elk -l app=filebeat-filebeat -w\n</code></pre> Output - filebeat is deployed as a DaemonSet (one on each worker node)<pre><code># NAME                      READY   STATUS              RESTARTS   AGE\n# filebeat-filebeat-m6hf4   1/1     Running             0          26s\n# filebeat-filebeat-72b79   1/1     Running             0          26s\n# NAME                               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n# daemonset.apps/filebeat-filebeat   2         2         2       2            2           &lt;none&gt;          3h4m\n</code></pre></p> </li> <li> <p>We have now installed Filebeat and it will start collecting logs from all Karbon deployed kubenetes nodes</p> </li> <li> <p>To verify if Filebeat is setup properly to receive logs from      kubernetes nodes and containers, check the data ingestion stats of     Elastisearch ClusterIP service</p> </li> <li> <p>Port-foward Elasticsearch Services IP to your local machine</p> <pre><code>k port-forward service/elasticsearch-master 9200:9200 &amp;\n</code></pre> </li> <li> <p>Run curl command to see the data indices ingestion details</p> <p><pre><code>curl -l \"localhost:9200/_cat/indices?pretty&amp;s=i\"\n</code></pre> <pre><code># The output looks as follows and data ingest details are in the last two columns\n# Observe the filebeat line\n# green open filebeat-7.10.0-2020.12.02-000001 ufD341lKTwin_jpknbOIyA 1 1 2089328   0     1gb 529.1mb\n</code></pre></p> </li> <li> <p>This confirms that we are ingesting data into Elasticsearch using filebeat</p> </li> </ol>"},{"location":"logging/logging/#install-kibana","title":"Install Kibana","text":"<ol> <li> <p>Run the following command to install Kibana visualisation GUI</p> <p><pre><code>helm install kibana elastic/kibana\n</code></pre> <pre><code># You will see the following output\n# NAME: kibana\n# LAST DEPLOYED: Wed Dec  2 10:47:10 2020\n# NAMESPACE: elk\n# STATUS: deployed\n# REVISION: 1\n# TEST SUITE: None\n</code></pre></p> </li> <li> <p>Now that we have installed all three necessary service in ELK stack,     let us confirm that they are all ready and running.</p> <pre><code>k get all\n</code></pre> </li> </ol>"},{"location":"logging/logging/#accessing-kibana-gui","title":"Accessing Kibana GUI","text":"<p>It is now time to visualise our work and logs.</p> <p>Note that the Kibana service is of type <code>Cluster IP</code>. Since we don't have a LoadBalancer in our environment, we need port-forward the ClusterIP service to our workstation LinuxMintVM.</p> <ol> <li> <p>Run the following command to port forward Kibana service. You are     able to find the port number by listing Kibana service.</p> <p><pre><code>k get svc/kibana-kibana\n</code></pre> Find the port number of the Kibana service<pre><code># output\n# service/kibana-kibana                   ClusterIP   172.19.118.48    &lt;none&gt;        5601/TCP            3h2m\n# Here the service port number is ``5601``\n</code></pre> Forward the port to your local machine<pre><code>k port-forward svc/kibana-kibana 5601:5601 &amp;\n</code></pre></p> </li> <li> <p>Open a browser window on the workstation and access the following URL</p> <pre><code>http://localhost:5601\n</code></pre> </li> <li> <p>You will see Kibana GUI</p> <p></p> </li> <li> <p>Click on the menu and select Observability &gt; Overview</p> <p></p> </li> <li> <p>On the Overview page, you can see the log rates per minute. This     gives you an overview of log production and injection rates.</p> <p></p> </li> <li> <p>Click on the menu and select Observability &gt; Logs</p> </li> <li> <p>You will be able to see logs streaming from various sources in your     kubernetes clusters</p> <p></p> </li> <li> <p>You are able to use keyword search and also perform highligthing of     text</p> <p></p> </li> <li> <p>Experiment with Stream Live and other options</p> </li> <li> <p>You have now successfully setup ELK stack and are able to view logs in Kibana</p> </li> </ol>"},{"location":"logging/logging/#cleanup","title":"Cleanup","text":"<p>Run the following commands to cleanup your ELK Stack implementation.</p> <pre><code>helm uninstall kibana helm uninstall filebeat helm uninstall Elasticsearch\n</code></pre> <p>To cleanup physical volumes configured as a part of this lab. You can use the following commands:</p> <p><pre><code># Change default namespace to ELK (just to make sure)\nk config set-context --current --namespace=elk\n</code></pre> <pre><code>k get pvc -n elk\n# Get the names of pvc for elasticsearch master statefulsets\n# NAME                                          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           AGE\n# elasticsearch-master-elasticsearch-master-0   Bound    pvc-baadcba4-5f26-44e0-93f6-e93dab7a5b82   30Gi       RWO            default-storageclass   59m\n# elasticsearch-master-elasticsearch-master-1   Bound    pvc-0ee313b4-cd3d-4d55-84cc-53225af92da5   30Gi       RWO            default-storageclass   59m\n</code></pre> <pre><code>k delete pvc &lt;pvc-NAME&gt;\n\n# Be careful not to delete any other pvc\n# It may take a while so please be patient\n# You can specify grace time out period to be   (this is ok in the lab enviroment)\n# k delete pvc &lt;pvc-NAME&gt; --force --grace-period=0\n# Example:\n# k delete pvc elasticsearch-master-elasticsearch-master-0\n# k delete pvc elasticsearch-master-elasticsearch-master-1\n# There is no requirement to delete PV as it will be automatically deleted as defined in the StorageClass settings.\n</code></pre></p>"},{"location":"logging/logging/#takeaways","title":"Takeaways","text":"<ul> <li>ELK Stack is open-source logging mechanism which can be easily     implemented in a kubernete environment</li> <li>ELK Stack is easily configurable for customer's requirements</li> <li>Design aspect is important in planning resource and retention     requirement for logs</li> <li>Open-source software like ELK Stack for logging management have     limited support but are used widely. Advise the customer of support     limitations in using these software</li> </ul>"},{"location":"logging/logging1/","title":"Loggin in Karbon","text":"<p>In this section we will explore logging that is already built into each and every Karbon cluster that is present.</p>"},{"location":"logging/logging1/#elastic-logging-kibana","title":"Elastic Logging Kibana","text":"<p>In this lab, we will run through setting up logging with ELK Stack in your Karbon deployed kubernetes cluster using Helm. If you haven't setup Helm, use these Helm instructions to deploy it in your Linux Tools VM.</p> <p>This setup will collect logs from all kubernets nodes deployed in Karbon kubernetes cluster.</p> <p>All logs for kubernetes nodes (Master, ETCD and Workers) are collected by a separate instance of Elastisearch in the <code>ntnx-system</code> namespace. This is deployed by default in all Karbon kubernetes clusters.</p> <p>In a production implementation make sure sufficient thought has been put in for design components of Elasticsearch in terms of:</p> <ul> <li>Log retention which directly affects storage requirements <ul> <li>Physical Volumes (PV) - provisioned by Nutanix Volumes</li> <li>In this implementation we will use a 30 GB PV (this is also customizable)</li> </ul> </li> <li>Log rotation</li> <li>Namespace requirements for ELK - it is best to have a separate namespace for logging implementation and define resource boundaries</li> </ul>"},{"location":"logging/logging1/#access-your-karbon-kubernetes-cluster","title":"Access your Karbon Kubernetes Cluster","text":"<ol> <li> <p>Logon to your Prism Central <code>https://&lt;PC VM IP&gt;:9440</code></p> <p>Note<p>If you haven't got a Karbon deployed kubernetes cluster in your HPOC, refer here to create a kubernetes cluster in Nutanix.</p> </p> </li> <li> <p>Go to Menu &gt; Services &gt; Karbon</p> <p></p> </li> <li> <p>Select your Karbon cluster</p> </li> <li> <p>Click on Actions &gt; Download Kubeconfig</p> <p></p> </li> <li> <p>Click on Copy the command to clipboard</p> </li> <li> <p>Paste the contents in your Linux Tools VM shell</p> </li> <li> <p>Run the following command to verify your connectivity and display     the nodes in the cluster</p> <pre><code>alias 'k=kubectl'\nk get nodes -o wide\n</code></pre> <p></p> </li> <li> <p>You can list the namespaces, storage claims, physical volumes and     physical volume claims using the following commands</p> <pre><code>k get ns k get sc,pv,pvc -n ntnx-system\nk get po -n ntnx-system\n</code></pre> You can see 80 GiB of space is allocated to collect all logs<pre><code>k get pvc -n ntnx-system\nNAME                                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           AGE\nelasticsearch-logging-data-elasticsearch-logging-0   Bound    pvc-cc122aa5-a283-4eee-8b8c-f2c323c84191   80Gi       RWO            default-storageclass   34m  prometheus-k8s-db-prometheus-k8s-0                   Bound    pvc-ac032a67-bed0-483c-920d-aa115e6a9dbe   30Gi       RWO            default-storageclass   30m\nprometheus-k8s-db-prometheus-k8s-1                   Bound    pvc-40b91314-e534-4ba4-9e9e-2f37b431210c   30Gi       RWO            default-storageclass   30m\n</code></pre> <p>Info<p>Nutanix Karbon has automatically provisioned these kubernetes resources so it is ready to use. You have the option to provision additional storage claims, physical volumes, etc by using the Karbon console or using kubectl with YAML files</p> </p> </li> </ol>"},{"location":"logging/logging1/#access-to-kibana-logging-portal","title":"Access to Kibana Logging Portal","text":""},{"location":"logging/logging1/#access-your-karbon-kubernetes-cluster_1","title":"Access your Karbon Kubernetes Cluster","text":"<ol> <li> <p>Logon to your Prism Central <code>https://&lt;PC VM IP&gt;:9440</code></p> <p>Note<p>If you haven't got a Karbon deployed kubernetes cluster in your HPOC, refer here to create a kubernetes cluster in Nutanix.</p> </p> </li> <li> <p>Go to Menu &gt; Services &gt; Karbon</p> <p></p> </li> <li> <p>Select your Karbon cluster</p> </li> <li> <p>Click on Add-on and select Logging</p> <p>This will open a new tab in your browser</p> <p></p> </li> <li> <p>In the Kibana tab, click on Log Trails</p> <p></p> <p>Any Karbon deployed cluster will collect logs from kubernetes nodes</p> </li> <li> <p>Experiment with searching for some keywords in the logs.</p> </li> </ol>"},{"location":"logging/logging1/#takeaways","title":"Takeaways","text":"<ul> <li>Karbon comes with logging enabled using the ELK stack</li> <li>Additional instances of ELK stack can be created to monitor additional deployed applications</li> </ul>"},{"location":"promgraf/promgraf/","title":"Monitoring in Nutanix Karbon","text":"<p>All kubernetes clusters deployed using Nutanix Karbon can be monitored using Prometheus.</p> <p>The data from Prometheus can be used with Grafana for visualisation.</p> <p>In this lab we will use Prometheus and Grafana to monitor our Karbon created Kubernetes cluster.</p>"},{"location":"promgraf/promgraf/#what-is-prometheus","title":"What is Prometheus?","text":"<p>Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. It is now a standalone open source project and maintained independently of any company. Prometheus uses a time series database to record all data uploaded by scrapers.</p> <p>Source: Prometheus</p>"},{"location":"promgraf/promgraf/#prometheus-architecture","title":"Prometheus Architecture","text":"<p>Prometheus has control and data planes.</p> <ul> <li>The control plane is used to manage and host the Prometheus server side components including a time series database (TSDB)</li> <li>The data plane collects metrics from scrapers (Prometheus targets)</li> </ul> <p>You are able to see the server, target and visualisation components of Prometheus in the following diagram:</p> <p></p> <p>Image Source: Prometheus Architecture</p> <p>Prometheus control plane runs in a kubernetes namespace <code>ntnx-system</code> of Karbon deployed kubernetes cluster.</p>"},{"location":"promgraf/promgraf/#what-is-grafana","title":"What is Grafana?","text":"<p>Grafana tool allows to query, visualise and alert on metrics and logs.</p> <p>Grafana has a pluggable data source model and supports Prometheus among various other data sources. Grafana has a rich ecosystem and integration with services from various cloud monitoring tools like Amazon Cloudwatch, MS Azure, etc.</p> <p>Source: Grafana</p> <p>You can think of Prometheus as a metrics input system for Grafana as shown below:</p> <p></p> <p>Grafana works in a pull system where it pulls data from a Prometheus source.</p> <p>Grafana application runs as a replica set in a Kubernetes. This pods in this replica set run in a highly available manner to provide access.</p>"},{"location":"visualise/visualise/","title":"Visualising Metrics in Karbon","text":"<p>In this section we will configure dashboards to visualise the metrics collected by Prometheus.</p> <p>As discussed in the previous section all Prometheus collected metrics can be queried using PromQL (Prometheus Query Language). In this section we will configure a custom dashboard which uses a PromQL query to get data from Prometheus data source.</p>"},{"location":"visualise/visualise/#configuring-a-custom-dashboard","title":"Configuring a Custom Dashboard","text":"<p>Let's check the load per cpu in the Karbon kubernetes nodes</p> <ol> <li> <p>In Grafana UI; Click on items(four little boxes) and +New Dashboard</p> <p></p> </li> <li> <p>Click on + Add new panel</p> </li> <li> <p>Confirm Prometheus is the data source</p> </li> <li> <p>In the Metrics drop-down enter the following</p> <p><pre><code>instance:node_load1_per_cpu:ratio\n</code></pre> </p> </li> <li> <p>In the New dashboard / Edit panel window paste the query from above in the Metrics field</p> </li> <li> <p>In the Panel Settings change the title of the panel to Load Per CPU</p> <p></p> </li> <li> <p>Click on Save and Apply on the top right hand corner of the pages</p> </li> <li> <p>Confirm creation of panel and dashboard in the main window</p> </li> <li> <p>(Optional Step) - Explore editing the panel, editing X and Y axis, try some aggregate functions by selecting the Panel and Edit options</p> <p></p> </li> </ol>"},{"location":"visualise/visualise/#configuring-a-grafanacom-dashboard","title":"Configuring a Grafana.com Dashboard","text":"<p>Grafana has an open community of dashboard developers who constantly upload dashboard to grafana.com. These can be used used in your environment as well by their identification numbers.</p> <p>In this section we will configure a grafana.com dashboard.</p> <ol> <li> <p>In Grafana; Click on + and Dashboard</p> </li> <li> <p>Click on Import</p> <p></p> </li> <li> <p>Enter <code>1860</code> in the id text box</p> </li> <li> <p>Click on Load</p> <p></p> </li> <li> <p>In the next screen, give a name for your dashboard (or keep existing one), select the Prometheus data source (that we configured in the previous session) and click on Import</p> <p></p> <p>Info<p>It may take up to 30 seconds for the dashboard to load as there are quite a few queries and graph rendering that has to happen in the background</p> </p> <p></p> </li> <li> <p>Explore the CPU/Memory/Net/Disk and (and other areas of interest) in the dashboard.</p> <p></p> <p>Here is a screenshot of the Storage Disk metrics</p> <p></p> </li> <li> <p>Try Grafana.com id <code>8588</code> by creating a new dashboard and explore</p> </li> <li> <p>Here are few other Grafana.com dashboards that you can import from to have additional visualisation of metrics exposed by your applications which Prometheus collects and stores.</p> <p></p> </li> </ol> <p>Nutanix Karbon comes with the Prometheus instance installed and collecting information about the kubernetes. You just have to use this as a data source and visualise it in Grafana. </p> <p>You have now completed this section of configuring dashboards in Grafana. </p>"}]}